{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Overview\n",
    "\n",
    "The Terry Stops problem aims to predict the outcome of police stops based on reasonable suspicion using a classification model. The model considers various factors such as presence of weapons, time of day, and possibly gender and race of both the officer and the subject. However, the use of race and gender data raises ethical concerns and the importance of avoiding bias and discrimination must be taken into consideration. The goal of this model is to improve the efficiency and fairness of law enforcement actions, but the agencies must also monitor and address any potential biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Business Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Problem\n",
    "The Terry Stops presents a business opportunity to improve the efficiency and fairness of law enforcement actions. By developing a predictive model that can assist officers in determining the likelihood of an arrest being made during a Terry Stop, the law enforcement agencies can make informed decisions and potentially reduce the number of false arrests and incidents of police misconduct. However, it is important to approach this problem with caution and transparency, considering the ethical concerns raised by the use of gender and race data. The goal is to provide a tool that can help improve policing, while avoiding biases and discrimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to build a classifier that can predict the outcome of a Terry Stop (whether an arrest was made or not) based on reasonable suspicion. This will be done by considering various factors such as the presence of weapons, time of day of the call, and other relevant information. The model will be designed to address the binary classification problem, with the goal of improving the efficiency and fairness of law enforcement actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 0bjectives\n",
    "* To create a predictive model for Terry Stops that accurately predicts the outcome of the stop (arrest made or not)\n",
    "* To take into consideration key factors such as the presence of weapons and the time of the call in the model\n",
    "* To ensure that the model is ethically sound and avoids any biases or discrimination related to gender and race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Understanding\n",
    "This dataset was provided by the City of Seattle and is managed by the Seattle Police Department. It was created on April 13, 2017 and last updated on February 6, 2023. The dataset contains **54873**, rows and **23** columns, each row representing a unique Terry Stop record as reported by the officer conducting the stop. The columns in the dataset include information about the subject of the stop, such as the perceived age group, perceived race, and perceived gender. \n",
    "\n",
    "The dataset also includes information about the officer, such as the officer's gender, race, and year of birth. Additionally, the dataset includes information about the resolution of the stop, any weapons found, the date and time the stop was reported, and information about the underlying Computer Aided Dispatch (CAD) event. The data is updated daily and is licensed under the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Requirements\n",
    "\n",
    "* Data Preparation -> Loading Libraries -> Loading data -> Descriptive Exploration -> Data Cleaning -> Exploratory Descriptive Analysis (EDA) -> Pre-processing Data\n",
    "\n",
    "* Modelling -> Train test split -> Logistic Regression -> K-Nearest -> Decision Tree -> Logistic Regression -> Random Forest\n",
    "    \n",
    "* Evaluation -> Classification Metrics -> Best Perfoming Model\n",
    "\n",
    "* Conclusion -> Best Model\n",
    "    \n",
    "* Recommendation -> Most imporatnt features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preparation\n",
    "\n",
    "*  Update the Stop Resolution column to either be arrested (1) or not arrested (0):\n",
    "*  Change the date column to datetime so we can work with it. Add in the month as a new column:\n",
    "*  Group weapons into firearms vs. non-firearms vs. no weapon:\n",
    "*  Change Officer year of bith to give the officer age:\n",
    "*  Drop columns that we are not going to need:\n",
    "\n",
    "* Converting categorical data to numeric format through label encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Loading Libraries -> \n",
    "ii) Loading data -> \n",
    "iii) Descriptive Exploration -> \n",
    "iv) Data Cleaning -> \n",
    "v) Exploratory Descriptive Analysis (EDA) -> \n",
    "vi) Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file to pandas data frame\n",
    "Tery_stops_df = pd.read_csv(\"data/Terry_Stops.csv\")\n",
    "\n",
    "# preview the first 3 rows\n",
    "Tery_stops_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the  23 columns, with a concise explanation of the information contained in each column :\n",
    "\n",
    "**Subject Age Group:** Subject Age Group (10 year increments) as reported by the officer.\n",
    "\n",
    "**Subject ID:** Key, generated daily, identifying unique subjects in the dataset using a character to character match of first name and last name. \"Null\" values indicate an \"anonymous\" or \"unidentified\" subject. Subjects of a Terry Stop are not required to present identification.\n",
    "\n",
    "**GO/SC Num:** General Offense or Street Check number, relating the Terry Stop to the parent report. This field may have a one to many relationship in the data.\n",
    "\n",
    "**Terry Stop ID:** Key identifying unique Terry Stop reports.\n",
    "\n",
    "**Stop Resolution:** Resolution of the stop as reported by the officer.\n",
    "\n",
    "**Weapon Type:** Type of weapon, if any, identified during a search or frisk of the subject. Indicates \"None\" if no weapons was found.\n",
    "\n",
    "**Officer ID:** Key identifying unique officers in the dataset.\n",
    "\n",
    "**Officer YOB:** Year of birth, as reported by the officer.\n",
    "\n",
    "**Officer Gender:** Gender of the officer, as reported by the officer.\n",
    "\n",
    "**Officer Race:** Race of the officer, as reported by the officer.\n",
    "\n",
    "**Subject Perceived Race:** Perceived race of the subject, as reported by the officer.\n",
    "\n",
    "**Subject Perceived Gender:** Perceived gender of the subject, as reported by the officer.\n",
    "\n",
    "**Reported Date:** Date the report was filed in the Records Management System (RMS). Not necessarily the date the stop occurred but generally within 1 day.\n",
    "\n",
    "**Reported Time:** Time the stop was reported in the Records Management System (RMS). Not the time the stop occurred but generally within 10 hours.\n",
    "\n",
    "**Initial Call Type:** Initial classification of the call as assigned by 911.\n",
    "\n",
    "**Final Call Type:** Final classification of the call as assigned by the primary officer closing the event.\n",
    "\n",
    "**Call Type:** How the call was received by the communication center.\n",
    "\n",
    "**Officer Squad:** Functional squad assignment (not budget) of the officer as reported by the Data Analytics Platform (DAP).\n",
    "\n",
    "**Arrest Flag:** Indicator of whether a \"physical arrest\" was made, of the subject, during the Terry Stop. Does not necessarily reflect a report of an arrest in the Records Management System (RMS).\n",
    "\n",
    "**Frisk Flag:** Indicator of whether a \"frisk\" was conducted, by the officer, of the subject, during the Terry Stop.\n",
    "\n",
    "**Precinct:** Precinct of the address associated with the underlying Computer Aided Dispatch (CAD) event. Not necessarily where the Terry Stop occurred.\n",
    "\n",
    "**Sector:** Sector of the address associated with the underlying Computer Aided Dispatch (CAD) event. Not necessarily where the Terry Stop occurred.\n",
    "\n",
    "**Beat:** Beat of the address associated with the underlying Computer Aided Dispatch (CAD) event. Not necessarily where the Terry Stop occurred.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.  Descriptive Exploration\n",
    "To summarizes the characteristics of a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data shape\n",
    "print(f\"This dataset has {Tery_stops_df.shape[0]} rows and {Tery_stops_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.  What are the datatype of columns ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check number of categorical and numerical columns\n",
    "def columns_dtypes(df):\n",
    "    num = len(df.select_dtypes(include=np.number).columns)\n",
    "    cat = len(df.select_dtypes(include='object').columns)\n",
    "    print(f\"Numerical columns: {num}\")\n",
    "    print(f\"Categorical columns: {cat}\")\n",
    "    \n",
    "# call the function  \n",
    "columns_dtypes(Tery_stops_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. Data set description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataset\n",
    "Tery_stops_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Most of the figures are too large to give a clear description but this before any scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Target column distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_counts(df, col_name):\n",
    "    # Count the number of unique values for the column \"Stop Resolution\"\n",
    "    value_counts = df[col_name].value_counts()\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.bar(value_counts.index, value_counts.values)\n",
    "\n",
    "    # Label the x and y axis\n",
    "    plt.title(\"Bar plot for the count of \" + col_name, fontsize=20)\n",
    "    plt.xlabel(col_name, fontsize=16)\n",
    "    plt.ylabel(\"Count\", fontsize=16)\n",
    "\n",
    "    # Add grid\n",
    "    plt.grid(True, linestyle='--')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# call the function    \n",
    "plot_value_counts(Tery_stops_df,\"Stop Resolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Field contact dominated the stop resolution as repoted by officer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4 Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to describe categorical columns\n",
    "def count_plot(df, x_col, y_col):\n",
    "    # Plot the count plot\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.countplot(x=x_col, hue=y_col, data=df)\n",
    "\n",
    "    # Label the x and y axis\n",
    "    plt.title(\"Count plot of \" + x_col + \" and \" + y_col, fontsize=20)\n",
    "    plt.xlabel(x_col, fontsize=16)\n",
    "    plt.ylabel(\"Count\", fontsize=16)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# call the function \n",
    "count_plot(Tery_stops_df, \"Subject Age Group\", \"Stop Resolution\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_plot(Tery_stops_df, \"Stop Resolution\",\"Officer Race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Data Cleaning\n",
    "\n",
    "Identifying and correcting or removing inaccuracies, inconsistencies, and irrelevant data from a dataset. Start by handling missing values, removing duplicates, correcting data format, and transforming variables to make the data ready for modelling and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see type of data\n",
    "Tery_stops_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. Missing and Duplicate Values\n",
    " a function to check duplicates and null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_missing(dataframe):\n",
    "    # calculate percentage of missing values\n",
    "    percent_missing = dataframe.isnull().mean().round(4) * 100\n",
    "    count_missing = dataframe.isnull().sum()\n",
    "    # calculate percentage of duplicate rows\n",
    "    percent_duplicates = dataframe.duplicated().mean() * 100\n",
    "    # create result dataframe\n",
    "    result = pd.DataFrame({'Missing Values %': percent_missing, \n",
    "                           'Missing Values Count': count_missing, \n",
    "                           'Duplicate Values %': percent_duplicates})\n",
    "    # find column with most missing values\n",
    "    if percent_missing.max() !=0:\n",
    "        column_most_missing = percent_missing.idxmax()\n",
    "        print(f\"{(column_most_missing).capitalize()} is the column with most missing values.\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No column with missing values\")\n",
    "    if percent_duplicates.max() !=0:\n",
    "        column_most_duplicates = percent_duplicates.idxmax()\n",
    "        print(\"Column with most duplicates:\",column_most_duplicates)\n",
    "    else:\n",
    "        print(\"No duplicates\")\n",
    "    return result\n",
    "\n",
    "check_duplicates_missing(Tery_stops_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_data = Tery_stops_df[Tery_stops_df.isnull().any(axis=1)]\n",
    "#null_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the Stop Resolution column to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical columns\n",
    "num_cols = Tery_stops_df.select_dtypes(include= np.number)\n",
    "\n",
    "# categorical columns\n",
    "cat_cols = Tery_stops_df.select_dtypes(include= 'object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0. Modelling\n",
    "\n",
    ">Is this a classification task? \n",
    "What models will we try?\n",
    "How do we deal with overfitting?\n",
    "Do we need to use regularization or not?\n",
    "What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "What loss functions will we use?\n",
    "What threshold of performance do we consider as successful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### how are our features distrubuted\n",
    "def dist_check(my_data):\n",
    "    num_cols = my_data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "    ncols = len(num_cols)\n",
    "    plt.figure(figsize=(20, int(22/4 * (ncols/4 + 1))))\n",
    "    for i, col in enumerate(num_cols):\n",
    "        plt.subplot(ncols//4 + 1, 4, i + 1)\n",
    "        sns.histplot(my_data[col], kde=True)\n",
    "        plt.title(col)\n",
    "dist_check(DataOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dtypes(Tery_stops_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_outliers_zscore(df, threshold=3):\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    numerical_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "    for col in numerical_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        z_scores = (df[col] - mean) / std\n",
    "        outliers = df[np.abs(z_scores) > threshold]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.boxenplot(df[col], ax=ax)\n",
    "        ax.scatter(x=outliers.index, y=outliers[col], color='red', s=50)\n",
    "        ax.set_title(f\"Outliers in {col.title()}\")\n",
    "        ax.set_xlabel(col)\n",
    "        plt.show()\n",
    "\n",
    "visualize_outliers_zscore(DataOne)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corre_plot(df, col=None):\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    corr = df.corr()\n",
    "    if col:\n",
    "        corr = corr[col].drop(col)\n",
    "        corr = corr[abs(corr) > 0.5].sort_values(ascending=False)\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm',linewidths=0.5)\n",
    "    plt.title(\"Correlation\", fontsize=18)\n",
    "    # show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
